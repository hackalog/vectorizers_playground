{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Word Embeddings\n",
    "\n",
    "The goal of this notebook is to use the tools in the [Tutte Institute ``vectorizers`` library](https://github.com/TutteInstitute/vectorizers) to construct word embeddings, and to compare those embeddings with the results from word2vec. We compare:\n",
    "* Gensim word2vec\n",
    "* 150 dimensional `word_vectorizer` (vectorizers library)\n",
    "* 300 dimensional `word_vectorizer` (vectorizers library)\n",
    "\n",
    "We will compare these techniques using some standard benchmarking.\n",
    "\n",
    "Rather than comparing against a pre-trained word2vec model, we will train all of our word embedding algorithms on a standard corpus, and then compare them. This provides a common \"start line\", and avoids the implicit  hidden cost (and advantage) of using a pre-trained model coming from labs or institutions with large compute and better data access. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amywooding/miniconda3/envs/vectorizers_playground/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_records': 1701,\n",
       " 'record_format': 'list of str (tokens)',\n",
       " 'file_size': 33182058,\n",
       " 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py',\n",
       " 'license': 'not found',\n",
       " 'description': 'First 100,000,000 bytes of plain text from Wikipedia. Used for testing purposes; see wiki-english-* for proper full Wikipedia datasets.',\n",
       " 'checksum': '68799af40b6bda07dfa47a32612e5364',\n",
       " 'file_name': 'text8.gz',\n",
       " 'read_more': ['http://mattmahoney.net/dc/textdata.html'],\n",
       " 'parts': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.info('text8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1002\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1002\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.2.min.js\": \"XypntL49z55iwGVUW4qsEu83zKL3XEcz0MjuGOQ9SlaaQ68X/g+k1FcioZi7oQAc\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.2.min.js\": \"bEsM86IHGDTLCS0Zod8a8WM6Y4+lafAL/eSiyQcuPzinmWNgNO2/olUF0Z2Dkn5i\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.2.min.js\": \"TX0gSQTdXTTeScqxj6PVQxTiRW8DOoGVwinyi1D3kxv7wuxQ02XkOxv0xwiypcAH\"};\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://unpkg.com/tabulator-tables@4.9.3/dist/js/tabulator.js\", \"https://unpkg.com/moment@2.27.0/moment.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.2.min.js\", \"https://unpkg.com/@holoviz/panel@^0.11.3/dist/panel.min.js\"];\n",
       "  var css_urls = [\"https://unpkg.com/tabulator-tables@4.9.3/dist/css/tabulator_simple.min.css\"];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1002\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.2.min.js\": \"XypntL49z55iwGVUW4qsEu83zKL3XEcz0MjuGOQ9SlaaQ68X/g+k1FcioZi7oQAc\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.2.min.js\": \"bEsM86IHGDTLCS0Zod8a8WM6Y4+lafAL/eSiyQcuPzinmWNgNO2/olUF0Z2Dkn5i\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.2.min.js\": \"TX0gSQTdXTTeScqxj6PVQxTiRW8DOoGVwinyi1D3kxv7wuxQ02XkOxv0xwiypcAH\"};\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://unpkg.com/tabulator-tables@4.9.3/dist/js/tabulator.js\", \"https://unpkg.com/moment@2.27.0/moment.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.2.min.js\", \"https://unpkg.com/@holoviz/panel@^0.11.3/dist/panel.min.js\"];\n  var css_urls = [\"https://unpkg.com/tabulator-tables@4.9.3/dist/css/tabulator_simple.min.css\"];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "\n",
    "from svd2vec import FilesIO\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "\n",
    "import vectorizers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import umap\n",
    "import umap.plot\n",
    "from bokeh.plotting import show\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "umap.plot.output_notebook()\n",
    "sns.set_palette('deep')\n",
    "sns.set(style=\"white\", context=\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#documents = FilesIO.load_corpus(\"text8\")\n",
    "#comes tokenized\n",
    "documents = list(api.load(\"text8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src.user.utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0e30675ad90e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_word2vec_formatting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src.user.utils'"
     ]
    }
   ],
   "source": [
    "from src.user.utils import save_word2vec_formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Vectorizer Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "word_vectorizer = vectorizers.TokenCooccurrenceVectorizer(\n",
    "    min_occurrences=50,\n",
    "    window_radii=(1, 15),\n",
    "    window_functions=(\"fixed\", \"variable\"),\n",
    "    kernel_functions=(\"geometric\", \"geometric\"),\n",
    "    kernel_args=({}, {\"offset\":1}),\n",
    "    mask_string=\"[##MASK##]\",\n",
    "    nullify_mask=True,\n",
    "    n_iter=4,\n",
    "    normalize_windows=True,\n",
    "    normalization=\"frequentist\",\n",
    ").fit(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 150 Dimensional Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "word_vectors = word_vectorizer.reduce_dimension(dimension=150, algorithm=\"randomized\")\n",
    "save_word2vec_format(\"./tcv_150_dim.word2vec\", word_vectorizer.token_label_dictionary_, word_vectors)\n",
    "model1 = Word2VecKeyedVectors.load_word2vec_format(\"./tcv_150_dim.word2vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 300 Dimensional Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "word_vectors = word_vectorizer.reduce_dimension(dimension=300, algorithm=\"randomized\")\n",
    "save_word2vec_format(\"./tcv_300_dim.word2vec\", word_vectorizer.token_label_dictionary_, word_vectors)\n",
    "model2 = Word2VecKeyedVectors.load_word2vec_format(\"./tcv_300_dim.word2vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if not os.path.isfile(\"gensim_w2v_50_window_10.word2vec\"):\n",
    "    g_w2v = Word2Vec(documents, vector_size=300, window=10, min_count=50, workers=16, sample=1e-3, epochs=30)\n",
    "    g_w2v.wv.save_word2vec_format(\"gensim_w2v_50_window_10.word2vec\")\n",
    "\n",
    "gensim_w2v = Word2VecKeyedVectors.load_word2vec_format(\"gensim_w2v_50_window_10.word2vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Similarity comparison plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_similarity(model, model_name, d=\"\\t\", n_bootstrap_samples=50):\n",
    "    result = {}\n",
    "    ok_vocab = {k.lower(): model.key_to_index[k] for k in reversed(model.index_to_key) if k != \"[##MASK##]\"}\n",
    "    original_vocab = model.key_to_index\n",
    "    model.key_to_index = ok_vocab\n",
    "    for benchmark in (\"wordsim353\", \"men_dataset\", \"mturk\", \"simlex999\", \"rarewords\"):\n",
    "        benchmark_path = FilesIO.path(f\"similarities/{benchmark}.txt\")\n",
    "        all_pairs = []\n",
    "        with open(benchmark_path) as f:\n",
    "            for line in f:\n",
    "                if line.startswith(\"#\"):\n",
    "                    continue\n",
    "                else:\n",
    "                    a, b, sim = [word.lower() for word in line.split(d)]\n",
    "                    all_pairs.append((a, b, float(sim)))\n",
    "        all_pairs = pd.DataFrame(all_pairs, columns=(\"word\", \"comparison\", \"similarity\"))\n",
    "        good_pairs = all_pairs[(all_pairs.word.isin(ok_vocab)) & (all_pairs.comparison.isin(ok_vocab))]\n",
    "        result[benchmark] = []\n",
    "        for i in range(n_bootstrap_samples):\n",
    "            sample = good_pairs.sample(len(all_pairs), replace=True)\n",
    "            similarity_gold = []\n",
    "            similarity_model = []\n",
    "            oov = 0\n",
    "            for a, b, sim in sample.itertuples(index=False):\n",
    "                if a not in ok_vocab or b not in ok_vocab:\n",
    "                    oov += 1\n",
    "                    continue\n",
    "                similarity_gold.append(sim)  # Similarity from the dataset\n",
    "                similarity_model.append(model.similarity(a, b))  # Similarity from the model\n",
    "                \n",
    "            spearman = scipy.stats.spearmanr(similarity_gold, similarity_model)[0]\n",
    "            pearson = scipy.stats.pearsonr(similarity_gold, similarity_model)[0]\n",
    "            kendalltau = scipy.stats.kendalltau(similarity_gold, similarity_model)[0]\n",
    "            oov_ratio = float(oov) / (len(similarity_gold) + oov)\n",
    "            \n",
    "            result[benchmark].append((spearman, pearson, kendalltau, oov_ratio))\n",
    "    model.key_to_index = original_vocab\n",
    "    \n",
    "    dfs = []\n",
    "    for benchmark in result:\n",
    "        df = pd.DataFrame(result[benchmark], columns=(\"spearman\", \"pearson\", \"kendalltau\", \"missed_word_ratio\"))\n",
    "        df[\"benchmark\"] = benchmark\n",
    "        dfs.append(df)\n",
    "    df = pd.concat(dfs)\n",
    "    \n",
    "    df = df.melt(\"benchmark\")\n",
    "    df.columns = (\"benchmark\", \"correlation_type\", \"correlation\")\n",
    "    df[\"model\"] = model_name\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = pd.concat([\n",
    "    compare_similarity(gensim_w2v, \"gensim\"),\n",
    "    #compare_similarity(word2vec_w2v, \"word2vec\"),\n",
    "    compare_similarity(model1, \"150 dim\"),\n",
    "    compare_similarity(model2, \"300 dim\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = results[\n",
    "    (results.benchmark.isin([\"men_dataset\", \"wordsim353\"])) &\n",
    "    (results.correlation_type.isin([\"spearman\", \"pearson\"]))\n",
    "].copy()\n",
    "df.benchmark = df.benchmark.map({\"wordsim353\": \"WordSim 353\", \"men_dataset\": \"MEN Word Similarity\"})\n",
    "plt.figure(figsize=(12,8));\n",
    "g = sns.catplot(\n",
    "    kind=\"swarm\", \n",
    "    x=\"model\", \n",
    "    y=\"correlation\", \n",
    "    col=\"benchmark\",\n",
    "    row=\"correlation_type\",\n",
    "    order=[\n",
    "        #\"word2vec\",\n",
    "        \"gensim\", \n",
    "        \"150 dim\",\n",
    "        \"300 dim\",\n",
    "    ],\n",
    "    data=df, \n",
    "    height=8, \n",
    "    alpha=0.33,\n",
    "    zorder=1,\n",
    "    s=10,\n",
    ")\n",
    "g.map_dataframe(\n",
    "    sns.pointplot, \n",
    "    x=\"model\", \n",
    "    y=\"correlation\", \n",
    "    col=\"benchmark\",\n",
    "    row=\"correlation_type\",\n",
    "    data=df,  \n",
    "    height=6, \n",
    "    aspect=1.3,\n",
    "    palette=\"dark\",\n",
    "    join=False,\n",
    "    order=[\n",
    "        #\"word2vec\",\n",
    "        \"gensim\", \n",
    "        \"150 dim\",\n",
    "        \"300 dim\",\n",
    "    ],\n",
    "    ci=\"sd\",\n",
    "    zorder=2,\n",
    "    scale=0.9,\n",
    ")\n",
    "g.set_titles(template=\"{col_name}\\n{row_name} correlation\")\n",
    "g.set_xticklabels([\n",
    "        #\"Google\\nWord2Vec\",\n",
    "        \"Gensim\\nWord2Vec\", \n",
    "        \"Vectorizers\\n150 dim\", \n",
    "        \"Vectorizers\\n300 dim\", \n",
    "])\n",
    "g.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some weirdness occasionally occurs due to our use of MASK values for uncommon vocab; fix this if required.\n",
    "if \"[##MASK##]\" in model1.index_to_key: model1.index_to_key.remove(\"[##MASK##]\")\n",
    "if \"[##MASK##]\" in model2.index_to_key: model2.index_to_key.remove(\"[##MASK##]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for benchmark in (\"wordsim353\", \"men_dataset\", \"mturk\", \"simlex999\", \"rarewords\"):\n",
    "    benchmark_path = FilesIO.path(f\"similarities/{benchmark}.txt\")\n",
    "    print(\"pearson correlation of\", os.path.basename(benchmark_path))\n",
    "    print(\"\\tgensim_w2v   \\t\\t\", gensim_w2v.evaluate_word_pairs(benchmark_path, delimiter=\"\\t\")[0][0])\n",
    "    #print(\"\\tword2vec_w2v \\t\\t\", word2vec_w2v.evaluate_word_pairs(benchmark_path, delimiter=\"\\t\")[0][0])\n",
    "    print(\"\\t150 dim      \\t\\t\", model1.evaluate_word_pairs(benchmark_path, delimiter=\"\\t\")[0][0])\n",
    "    print(\"\\t300 dim      \\t\\t\", model2.evaluate_word_pairs(benchmark_path, delimiter=\"\\t\")[0][0])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Analogy comparison plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_analogies(model, model_name, n_bootstrap_samples=20):\n",
    "    result = {}\n",
    "    ok_vocab = {k.lower(): model.key_to_index[k] for k in reversed(model.index_to_key) if k != \"[##MASK##]\"}\n",
    "    original_vocab = model.key_to_index\n",
    "    model.key_to_index = ok_vocab\n",
    "    for benchmark in (\"questions-words\", \"msr\"):\n",
    "        benchmark_path = FilesIO.path(f\"analogies/{benchmark}.txt\")\n",
    "        analogies = []\n",
    "        with open(benchmark_path) as f:\n",
    "            for line in f:\n",
    "                if line.startswith(': '):\n",
    "                    continue\n",
    "                else:\n",
    "                    a, b, c, expected = [word.lower() for word in line.split()]\n",
    "                    analogies.append((a, b, c, expected))\n",
    "        analogies = pd.DataFrame(analogies, columns=(\"from\", \"to\", \"as_from\", \"as_to\"))\n",
    "        good_analogies = analogies[\n",
    "            (analogies[\"from\"].isin(ok_vocab)) &\n",
    "            (analogies.to.isin(ok_vocab)) &\n",
    "            (analogies.as_from.isin(ok_vocab)) &\n",
    "            (analogies.as_to.isin(ok_vocab))\n",
    "        ]\n",
    "        result[benchmark] = []\n",
    "        for i in range(n_bootstrap_samples):\n",
    "            n_correct = 0\n",
    "            n_incorrect = 0\n",
    "            oov = 0\n",
    "            sample = good_analogies.sample(len(good_analogies), replace=True)\n",
    "            for a, b, c, expected in sample.itertuples(index=False):\n",
    "                if a not in ok_vocab or b not in ok_vocab or c not in ok_vocab or expected not in ok_vocab:\n",
    "                    oov += 1\n",
    "                    continue\n",
    "                \n",
    "                ignore = {a, b, c}  # input words to be ignored\n",
    "                predicted = None\n",
    "                sims = model.most_similar(positive=[b, c], negative=[a], topn=5)\n",
    "                for element in sims:\n",
    "                    predicted = element[0].lower()\n",
    "                    if predicted in ok_vocab and predicted not in ignore:\n",
    "                        if predicted == expected:\n",
    "                            break\n",
    "                            \n",
    "                if predicted == expected:\n",
    "                    n_correct += 1\n",
    "                else:\n",
    "                    n_incorrect += 1\n",
    "                    \n",
    "            accuracy = n_correct / (n_correct + n_incorrect)\n",
    "            missed = oov / (n_correct + n_incorrect + oov)\n",
    "            result[benchmark].append((accuracy, missed))\n",
    "            \n",
    "    dfs = []\n",
    "    for benchmark in result:\n",
    "        df = pd.DataFrame(result[benchmark], columns=(\"accuracy\", \"missing_rate\"))\n",
    "        df[\"benchmark\"] = benchmark\n",
    "        dfs.append(df)\n",
    "    df = pd.concat(dfs)\n",
    "    \n",
    "    df = df.melt(\"benchmark\")\n",
    "    df.columns = (\"benchmark\", \"score_type\", \"score\")\n",
    "    df[\"model\"] = model_name\n",
    "    \n",
    "    return df\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy_results = pd.concat([\n",
    "    compare_analogies(gensim_w2v, \"gensim\"),\n",
    "    #compare_analogies(word2vec_w2v, \"word2vec\"),\n",
    "    compare_analogies(model1, \"150 dim\"),\n",
    "    compare_analogies(model2, \"300 dim\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = analogy_results[\n",
    "    (analogy_results.score_type.isin([\"accuracy\"]))\n",
    "].copy()\n",
    "df.benchmark = df.benchmark.map({\"questions-words\": \"Google Analogies\", \"msr\": \"Microsoft Research Analogies\"})\n",
    "plt.figure(figsize=(16,8))\n",
    "g = sns.catplot(\n",
    "    kind=\"swarm\", \n",
    "    x=\"model\", \n",
    "    y=\"score\", \n",
    "    col=\"benchmark\",\n",
    "    order=[\n",
    "        #\"word2vec\",\n",
    "        \"gensim\", \n",
    "        \"150 dim\",\n",
    "        \"300 dim\",\n",
    "    ],\n",
    "    data=df, \n",
    "    height=8, \n",
    "    alpha=0.33,\n",
    "    zorder=1,\n",
    "    s=10,\n",
    ")\n",
    "g.map_dataframe(\n",
    "    sns.pointplot, \n",
    "    x=\"model\", \n",
    "    y=\"score\", \n",
    "    col=\"benchmark\",\n",
    "    data=df,  \n",
    "    height=6, \n",
    "    aspect=1.3,\n",
    "    palette=\"dark\",\n",
    "    join=False,\n",
    "    order=[\n",
    "        #\"word2vec\",\n",
    "        \"gensim\", \n",
    "        \"150 dim\",\n",
    "        \"300 dim\",\n",
    "    ],\n",
    "    ci=\"sd\",\n",
    "    zorder=2,\n",
    "    scale=0.9,\n",
    ")\n",
    "g.set_titles(template=\"{col_name}\")\n",
    "g.set_xticklabels([\n",
    "        #\"Google\\nWord2Vec\",\n",
    "        \"Gensim\\nWord2Vec\", \n",
    "        \"Vectorizers\\n150 dim\", \n",
    "        \"Vectorizers\\n300 dim\", \n",
    "])\n",
    "g.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some weirdness occasionally occurs due to our use of MASK values for uncommon vocab; fix this if required.\n",
    "if \"[##MASK##]\" in model1.index_to_key: model1.index_to_key.remove(\"[##MASK##]\")\n",
    "if \"[##MASK##]\" in model2.index_to_key: model2.index_to_key.remove(\"[##MASK##]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for benchmark in (\"questions-words\", \"msr\"):\n",
    "    benchmark_path = FilesIO.path(f\"analogies/{benchmark}.txt\")\n",
    "    print(\"analogies success rate of\", os.path.basename(benchmark_path))\n",
    "    print(\"\\tgensim_w2v   \\t\\t\", gensim_w2v.evaluate_word_analogies(benchmark_path)[0])\n",
    "    #print(\"\\tword2vec_w2v \\t\\t\", word2vec_w2v.evaluate_word_analogies(benchmark_path)[0])\n",
    "    print(\"\\t150 dim      \\t\\t\", model1.evaluate_word_analogies(benchmark_path)[0])\n",
    "    print(\"\\t300 dim      \\t\\t\", model2.evaluate_word_analogies(benchmark_path)[0])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The 150 dimensional word embedding seems like great value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Visualization via UMAP Embedding \n",
    "We'll just look at the 150 dimensional word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hover_data = pd.DataFrame()\n",
    "hover_data['vocab'] = model2.index_to_key\n",
    "mapper = umap.UMAP(n_neighbors=25, metric = 'cosine', random_state=42).fit(model1.vectors[:-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = umap.plot.interactive(\n",
    "    mapper, \n",
    "    hover_data=hover_data, \n",
    "    point_size=5, \n",
    "    values=np.log(word_vectorizer._token_frequencies_), \n",
    "    interactive_text_search=True, \n",
    "    interactive_text_search_alpha_contrast=0.99)\n",
    "show(plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vectorizers_playground] *",
   "language": "python",
   "name": "conda-env-vectorizers_playground-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
